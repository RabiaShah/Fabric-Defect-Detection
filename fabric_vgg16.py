# -*- coding: utf-8 -*-
"""Fabric-VGG16-New.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mcizhOiWUmKSWB1L5hvTQJNziRnEiYP4
"""

pip install bayesian-optimization

import os
import tensorflow as tf
from keras.layers import Input, Lambda, Dense, Flatten
from keras.models import Model
from keras.applications.vgg16 import VGG16
from keras.applications.vgg16 import preprocess_input
from keras.preprocessing import image
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from tensorflow.keras.utils import plot_model
from bayes_opt import BayesianOptimization
from keras import optimizers
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
import itertools
import numpy as np
from glob import glob
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

physical_devices = tf.config.experimental.list_physical_devices('GPU')
print("Num GPUs Available: ", len(physical_devices))

train_path = '/content/drive/MyDrive/Thesis/Old Fabric Defect Dataset/train'
valid_path = '/content/drive/MyDrive/Thesis/Old Fabric Defect Dataset/valid'
test_path = '/content/drive/MyDrive/Thesis/Old Fabric Defect Dataset/test'
len(os.listdir('/content/drive/MyDrive/Thesis/Old Fabric Defect Dataset/test/hole'))

"""# Pre-processing and model training"""

train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(
    directory=train_path, target_size=(224,224), batch_size=10, class_mode="sparse")
valid_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(
    directory=valid_path, target_size=(224,224), batch_size=10, class_mode="sparse")
test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(
    directory=test_path, target_size=(224,224), batch_size=10, shuffle=False, class_mode="sparse")

IMAGE_SIZE = [224, 224]
test_batches.class_indices

vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)

for layer in vgg.layers:
  layer.trainable = False

x = Flatten()(vgg.output)
prediction = Dense(units=3, activation='softmax')(x)
model = Model(inputs=vgg.input, outputs=prediction)
model.summary()

plot_model(model, to_file='vgg_architecture.png')

adam = optimizers.Adam()
model.compile(loss='sparse_categorical_crossentropy',
              optimizer=adam,
              metrics=['accuracy'])

model_history=model.fit(
  train_batches,
  validation_data=test_batches,
  epochs=40,
  # steps_per_epoch=5,
  # validation_steps=32,
  verbose=2)

max_accuracy = max(model_history.history['val_accuracy'])
epoch = model_history.history['val_accuracy'].index(max_accuracy)
print('Max Accuracy: ',max_accuracy)
print('Epoch: ', epoch)

plt.figure(figsize=(8, 6))
for c in ['loss', 'val_loss']:
    plt.plot(model_history.history[c], label=c)
plt.legend()
plt.xlabel('Epoch')
plt.ylabel('Average Negative Log Likelihood')
plt.title('Training and Validation Losses')

plt.figure(figsize=(8, 6))
for c in ['accuracy', 'val_accuracy']:
    plt.plot(model_history.history[c], label=c)
plt.legend()
plt.xlabel('Epoch')
plt.ylabel('Average Accuracy')
plt.title('Training and Validation Accuracy')

"""# **Precision, Recall, and F1 Score**"""

y_true = test_batches.classes
predictions = model.predict(x=test_batches, steps=len(test_batches), verbose=0)
y_pred = predictions.argmax(axis=1)

#calculating precision and reall
precision = precision_score(y_true, y_pred, average='micro')
recall = recall_score(y_true, y_pred, average='micro')
F1 = f1_score(y_true, y_pred, average='micro')

print('Precision: ',precision)
print('Recall: ',recall)
print('F1 Score: ',F1)

"""# **Bayesian Optimizer**"""

# Define the objective function for Bayesian optimization
def objective(learning_rate, dropout_rate):
    # Update the model with the new hyperparameters
    optimizer.learning_rate = learning_rate
    model.layers[-5].rate = dropout_rate
    model.layers[-7].rate = dropout_rate

    # Train the model
    history = model.fit(x=train_batches,
            validation_data=test_batches,
            epochs=20,
            verbose=2
)

    # Return the validation accuracy for optimization
    return max(history.history['val_accuracy'])

# Define the parameter ranges for Bayesian optimization
pbounds = {'learning_rate': (1e-6, 1e-2),
           'dropout_rate': (0.0, 0.5)}

# Run Bayesian optimization
optimizer = BayesianOptimization(f=objective, pbounds=pbounds, verbose=2)
optimizer.maximize(init_points=2, n_iter=2)

# Get the best hyperparameters
best_params = optimizer.max['params']
best_learning_rate = best_params['learning_rate']
best_dropout_rate = best_params['dropout_rate']

# Update the model with the best hyperparameters
optimizer.learning_rate = best_learning_rate
model.layers[-5].rate = best_dropout_rate
model.layers[-7].rate = best_dropout_rate

# Train the model with the best hyperparameters
history = model.fit(x=train_batches,
            validation_data=test_batches,
            epochs=20,
            verbose=2
)

plt.figure(figsize=(8, 6))
for c in ['loss', 'val_loss']:
    plt.plot(model_history.history[c], label=c)
plt.legend()
plt.xlabel('Epoch')
plt.ylabel('Average Negative Log Likelihood')
plt.title('Training and Validation Losses')

plt.figure(figsize=(8, 6))
for c in ['accuracy', 'val_accuracy']:
    plt.plot(model_history.history[c], label=c)
plt.legend()
plt.xlabel('Epoch')
plt.ylabel('Average Accuracy')
plt.title('Training and Validation Accuracy')

"""# **Precision, Recall, and F1 Score**"""

y_true = test_batches.classes
predictions = model.predict(x=test_batches, steps=len(test_batches), verbose=0)
y_pred = predictions.argmax(axis=1)

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
            horizontalalignment="center",
            color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

#calculating precision and reall
cm = confusion_matrix(y_true, y_pred)
true_pos = np.diag(cm)
false_pos = np.sum(cm, axis=0) - true_pos
false_neg = np.sum(cm, axis=1) - true_pos

precision = np.average(true_pos / (true_pos + false_pos))
recall = np.average(true_pos / (true_pos + false_neg))
F1 = 2 * (precision * recall) / (precision + recall)

print('Precision: ',precision)
print('Recall: ',recall)
print('F1 Score: ',F1)

plot_confusion_matrix(cm=cm, classes=['Hole','Horizontal','Vertical'], title='Confusion Matrix')

